---
title: "Homework4"
author: "Shiyu Wang"
date: "April 15, 2017"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Problem 1

##### (a)

```
For x<=ξ, f1(x) has coefficients a1=β0,b1=β1,c1=β2,d1=β3a1=β0,b1=β1,c1=β2,d1=β3.

```

##### (b)
```

For x>ξ, f(x) has the form of:
β0+β1x+β2x^2+β3^3+β4(x−ξ)^3
=β0+β1x+β2x^2+β3x^3+β4(x^3−3x^2ξ+3xξ^2−ξ^3)
=(β0−β4ξ^3)+(β1+3β4ξ^2)x+(β2−3β4ξ)x^2+(β3+β4)x^3

So 
a2=β0−β4ξ^3 
b2=β1+3β4ξ^2 
c2=β2−3β4ξ
d2=β3+β4
```

##### (c)

```
f1(ξ)=β0+β1ξ+β2ξ^2+β3ξ3
f2(ξ)=(β0−β4ξ^3)+(β1+3β4ξ^2)ξ+(β2−3β4ξ)ξ^2+(β3+β4)ξ^3
=β0−β4ξ3+β1ξ+3β4ξ3+β2ξ2−3β4ξ3+β3ξ3+β4ξ3
=β0+β1ξ+β2ξ^2+3β4ξ^3−3β4ξ^3+β3ξ^3+β4ξ^3−β4ξ^3
=β0+β1ξ+β2ξ^2+β3
```

##### (d)

```
f′(x)=b1+2c1x+3d1x^2
f′1(ξ)=β1+2β2ξ+3β3ξ^2
f′2(ξ)=β1+3β4ξ^2+2(β2−3β4ξ)ξ+3(β3+β4)ξ^2
=β1+3β4ξ^2+2β2ξ−6β4ξ^2+3β3ξ^2+3β4ξ^2
=β1+2β2ξ+3β3ξ^2+3β4ξ^2+3β4ξ^2−6β4ξ^2
=β1+2β2ξ+3β3ξ^2
```

##### (e)

```
f′′(x)=2c1+6d1x
f′′1(ξ)=2β2+6β3ξ
f′′2(ξ)=2(β2−3β4ξ)+6(β3+β4)ξ
=2β2+6β3ξ

```
#### Problem 2

```{r}
x = -2:2
y = 1 + x + -2 * (x-1)^2 * I(x>1)
plot(x, y)
```

#### Problem 3

##### (a)
```
Code is provided at the end of the report in appendix with python 2.7
```

##### (b)
```
Accuracy = 68.06 
Details of process can be found in appendix code
```

##### (c)
```
Accuracy  66.20
Details of process can be found in appendix code
LDA and Naive Bayes have similar accuracy because LDA is also based on Naive Bayes to separate
```
#### problem 4

##### (a)
```{r}
# read the data
SAheart <- read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data", sep=",",head=T,row.names=1)
# set the seed
set.seed(1)

library(ISLR)
attach(SAheart)
Present = ifelse(chd==1, "Yes", "No")
SAheart = data.frame(SAheart, Present)
train <- sample(1:462,231)
valid <- c(1:462)[-train]
SAheart$famhist <- as.character(SAheart$famhist)
SAheart$famhist[which(SAheart$famhist=="Present")] <- "1"
SAheart$famhist[which(SAheart$famhist=="Absent")] <- "0"
SAheart$famhist <- as.numeric(SAheart$famhist)
# create training and validation sets
SAheart_train <- SAheart[train,]
SAheart_train <- SAheart_train[,-10]
SAheart_valid <- SAheart[valid,]
SAheart_valid <- SAheart_valid[,-10]
```

##### (b)
```{r}
# fit a tree to the training data
# building tree model with all predictors
# It has 24 terminal nodes.
# Training error is 0.130
library(tree)
treeModel = tree(Present~., SAheart_train)
summary(treeModel)
```

##### c)
```{r}
treeModel
# Let's pick 822, it means if adiposity is the predictor, and when its value is greater than 31.18, 
# there're 7 data points belongs to this region. 
# The prediction at this node is chd=1, 14.286% of data points have cha value to be 0, and the rest of them are 1. 
# A * in the line denotes that this is in fact a terminal node
```


##### d)
```{r}
plot(treeModel)
text(treeModel, pretty = 0)
```

###### e)
```{r}
treePred = predict(treeModel, SAheart_valid, type="class")
table(SAheart_valid$Present, treePred)
```
Error rate is (42+41)/231 = 35.93%

##### f)
```{r}
treeCV = cv.tree(treeModel, FUN=prune.misclass)
names(treeCV)
treeCV
```
The tree with 6 terminal nodes results in the lowest cross-validation error rate, 78

##### g)
```{r}
plot(treeCV$size ,treeCV$dev ,type="b", xlab="Tree Size", ylab="CV classification Error Rate")
```

##### h)
When size = 6, result for cross-validation error rate is lowest.

##### i)
```{r}
treePrune =prune.misclass(treeModel ,best = 6)
```


##### j)
```{r}
summary(treePrune)
```
Misclassification error rate is 23.38%, which is higher than before.

##### k)
```{r}
treePrunePred = predict(treePrune, SAheart_valid, type="class")
table(SAheart_valid$Present, treePrunePred)
```
Test error rate is now (27+37)231 = 27.71%, which is lower than before.

##### (4b i & ii)
```{r}
library(randomForest)
library(caret)

train.errors = rep(0, 100)
test.errors = rep(0, 100)
for (i in 1:100) {
 tree.bag = randomForest(Present~., SAheart_train, mtry=9, ntree=i, importance=TRUE)
 yhat.bag = predict(tree.bag, SAheart_train)
 yhat.bag.test = predict(tree.bag, SAheart_valid)
 train.errors[i] = confusionMatrix(yhat.bag, SAheart_train$Present)$overall[1]
 test.errors[i] = confusionMatrix(yhat.bag.test, SAheart_valid$Present)$overall[1]
}
plot(train.errors, xlab="Number of Trees", ylab="% of correct predictions", main="Bagging on the training set")
plot(test.errors, xlab="Number of Trees", ylab="% of correct predictions", main="Bagging on the testing set")
```

##### (4b iii)

When B increase the accuracy on the training set is keep increasing, but it is not true for valid set. From the plot, we can see it may cause overfit if B is too large. 
##### (4c i & ii)
```{r}

train.errors1 = rep(0, 100)
train.errors2 = rep(0, 100)
test.errors1 = rep(0, 100)
test.errors2 = rep(0, 100)
for (i in 1:100) {
  rf1 = randomForest(Present~., SAheart_train, mtry=6, ntree=i, importance = TRUE)
  rf2 = randomForest(Present~., SAheart_train, mtry=3, ntree=i, importance = TRUE)
 yhat.rf1 = predict(rf1, SAheart_train)
 yhat.rf2 = predict(rf2, SAheart_train)
 yhat.rf1.test = predict(rf1, SAheart_valid)
 yhat.rf2.test = predict(rf2, SAheart_valid)
 train.errors1[i] = confusionMatrix(yhat.rf1, SAheart_train$Present)$overall[1]
 test.errors1[i] = confusionMatrix(yhat.rf1.test, SAheart_valid$Present)$overall[1]
 train.errors2[i] = confusionMatrix(yhat.rf2, SAheart_train$Present)$overall[1]
 test.errors2[i] = confusionMatrix(yhat.rf2.test, SAheart_valid$Present)$overall[1]
}
x=seq(1, 100)
plot(x, train.errors1, xlab="Number of Trees", ylab="% of correct predictions", type="p", main="Random Forest m=6 & m=3 on the training set", col="blue")
points(x, train.errors2, col="red")

plot(x, test.errors1, xlab="Number of Trees", ylab="% of correct predictions", type="p", main="Random Forest m=6 & m=3 on the validation set", col="blue")
points(x, test.errors2, col="red")
```
##### (4c iii)

This is similar as in Bagging. As we can see the training set accuracy is quickly increasing towards to 100%. However, it also cause overfit which doesn't improve accuracy of valid set prediction much after B > 20.

##### (4d)
```{r}
# i & ii
library(gbm)
pows = seq(-3, -1, by=0.1)
lambdas = 10^pows
length.lambdas = length(lambdas)
train.errors3 = rep(0, length.lambdas)
test.errors3 = rep(0, length.lambdas)
SAheart_train <- SAheart[train,]
SAheart_train <- SAheart_train[,-11]
SAheart_validation <- SAheart[valid,]
SAheart_validation <- SAheart_validation[,-11]
for (i in 1:length.lambdas) {
   tree.boost = gbm(chd~., data = SAheart_train, distribution = "bernoulli", n.trees = 1000, shrinkage = i, verbose = F)
   yhat.boost = predict(tree.boost, SAheart_train, n.trees = 1000,  type="response")
   yhat.boost.test = predict(tree.boost, SAheart_validation, n.trees = 1000,  type="response")
   yhat.boost1 = ifelse(yhat.boost>=0.5, 1, 0)
   yhat.boost1.test = ifelse(yhat.boost.test>=0.5, 1, 0)
   train.errors3[i] = confusionMatrix(yhat.boost1, SAheart_train$chd)$overall[1]
   test.errors3[i] = confusionMatrix(yhat.boost1.test, SAheart_validation$chd)$overall[1]
}
plot(train.errors3, xlab="Shrinkage parameter lambda", ylab="% of correct predictions", main="Boosting on the training set")
plot(test.errors3,xlab="Shrinkage parameter lambda", ylab="% of correct predictions", main="Boosting on the validation set")
#While the lambda increases from 0.001 to 0.1, the accuracy is decresing, this is because the slower the tree grows, the more accuracy the prediction would be.
```

##### (4c)

```{r}
tree.boost = gbm(chd~., data = SAheart_train, distribution = "bernoulli", n.trees = 1000, shrinkage = 0.01, verbose = F)
summary(tree.boost)
```
ldl, age and tobacco are highly correlated to chd.


#### Problem 5

##### (a)
```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
```

##### (b)

(2,2), (4,4) \ (2,1), (4,3) \ => (2,1.5), (4,3.5) \ b = (3.5 - 1.5) / (4 - 2) = 1 \ a = X2 - X1 = 1.5 - 2 = -0.5
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
```

##### (c)

0.5 - X1 + X2 > 0

##### (d)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```

##### (e)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
arrows(2, 1, 2, 1.5)
arrows(2, 2, 2, 1.5)
arrows(4, 4, 4, 3.5)
arrows(4, 3, 4, 3.5)
```

##### (f)
A slight movement of observation #7 (4,1) blue would not have an effect on the maximal margin hyperplane because it is out of margin and far from it.

##### (g)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.8, 1)
```
-0.8 - X1 + X2 > 0

##### (h)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(4), c(2), col = c("red"))
```

#### Problem 6

##### (a)

```{r}
data= read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data", sep=",",head=T,row.names=1)
data$chd <- factor(data$chd)
set.seed(1)
sub <- sample(nrow(data), floor(nrow(data)/2))
train<-data[sub,]
valid<-data[-sub,]
head(train)
library(caret)
library(ggplot2)
library(e1071)
library(MASS)
control <- trainControl(method="cv", number=12)
metric <- "Accuracy"
```

```{r}
# Linear Kernel
# Train
model.svm <- train(chd~., data=train, method="svmLinear", metric=metric, trControl=control)
prediction.svm <- predict(model.svm, train)
confusionMatrix(prediction.svm, train$chd)$overall[1]
# Valid
model.svm <- train(chd~., data=train, method="svmLinear", metric=metric, trControl=control)
prediction.svm <- predict(model.svm, valid)
confusionMatrix(prediction.svm, valid$chd)$overall[1]

# Radial Kernel
# Train
model.svm <- train(chd~., data=train, method="svmRadial", metric=metric, trControl=control)
prediction.svm <- predict(model.svm, train)
confusionMatrix(prediction.svm, train$chd)$overall[1]
# Valid
model.svm <- train(chd~., data=train, method="svmRadial", metric=metric, trControl=control)
prediction.svm <- predict(model.svm, valid)
confusionMatrix(prediction.svm, valid$chd)$overall[1]
```

```{r}
# ROC
#Tainning
library(ROCR)
rocplot=function(pred, truth, ...){
  predob = prediction(pred, truth)
  pref = performance(predob, "tpr", "fpr")
  plot(pref,...)
}

svmfit.opt=svm(chd~., data=train, kernel="radial", gamma=0.01, cost=1, decision.values=T)
fitted=attributes(predict(svmfit.opt, train, decision.values=TRUE))$decision.values
par(mfrow=c(1, 2))

rocplot(fitted, valid$chd, main="Test Data")
pred = prediction(fitted, valid$chd)
unlist(attributes(performance(pred, "auc"))$y.values)
```

```{r}
data= read.table("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data", sep=",",head=T,row.names=1)
data$famhist = as.numeric(data$famhist)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)

scaled.data <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
set.seed(1)
sub <- sample(nrow(scaled.data), floor(nrow(scaled.data)/2))
train<-scaled.data[sub,]
valid<-scaled.data[-sub,]
head(train)
```

```{r}
feats <- names(scaled.data[,c(1:9)])

# Concatenate strings
f <- paste(feats,collapse=' + ')
f <- paste('chd ~',f)

# Convert to formula
f <- as.formula(f)

f
```

```{r}
library(neuralnet)
nn <- neuralnet(f,data=train,hidden=c(10, 10, 10),linear.output=FALSE)
# Compute Predictions off Test Set
predicted.nn.values <- compute(nn,valid[,c(1:9)])

# Check out net.result
print(head(predicted.nn.values$net.result))
```



```{r}
plot(nn)
predicted.nn.values$net.result <- sapply(predicted.nn.values$net.result,round,digits=0)
table(valid$chd,predicted.nn.values$net.result)
# Accuracy = (125+38)/231 = 70.56%
```

#### Problem 7
Because our training and valid sets is randomly picked, we can't directly compare the accuracy of this homework to the one in homework3. roughly Speaking, they are performed similar because this problem is a simple 2 class classiﬁcation. I believe one of the reasons to cause them slightly diﬀerent is the random training and validation dataset. 

•Logistic regression and Nearal Network doesn't perform well because we didn’t check the correlationship between predictors.Andfrom the pair(train), we can see there are correlation between parameters.

•One drawback of diagonal LDA, Naive Bayes and SVM is that it depends on all of the features. This is the reason it can’tperform better in this problem.

•Logistic with Lasso shrunk the dataset to ﬁlter out the less related predictorswith zero coef. If we test diﬀerent number of predictors, the result possibly become even better.

• Nearest shrunken centroids only depends on a subset of the features, for reasons of accuracy andinterpret ability. With an accurate threshold with less err rate, it performs well in this problem

#### Appendix
```
import csv
import random
import math
import operator
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA


random.seed(123)


# loadDataset and create training set and test set
def load_data(filename, split, trainingSet, testSet):
    with open(filename, 'rb') as csvfile:
        lines = csv.reader(csvfile)
        dataset = list(lines)
        for x in range(len(dataset) - 1):
            for y in range(9):
                dataset[x][y] = float(dataset[x][y])
            if random.random() < split:
                trainingSet.append(dataset[x])
            else:
                testSet.append(dataset[x])


# separate class of chd for '0' & '1'
def separate(dataset):
    separated = {}
    for i in range(len(dataset)):
        vector = dataset[i]
        if (vector[-1] not in separated):
            separated[vector[-1]] = []
        separated[vector[-1]].append(vector)
    return separated


# find euclidean distance between two instances (rows)
def get_distance(instance1, instance2, length):
    distance = 0
    for x in range(length):
        distance += pow((instance1[x] - instance2[x]), 2)
    return math.sqrt(distance)


# find prior probability for each class
def priorProb(dict, dataset):
    total = len(dataset)
    prior = {}
    for k in dict:
        if (k == '1'):
            class1 = len(dict[k])
            prior1 = float(class1) / float(total)
            prior[k] = prior1
            print(prior1)
        else:
            class0 = len(dict[k])
            prior0 = float(class0) / float(total)
            prior[k] = prior0
            print(prior0)
    return prior


# get the most similar neighbors for a test instance
def get_neighbours(train, test, width):
    distances = []
    length = len(test) - 1
    for x in range(len(train)):
        dist = get_distance(train[x], test, length)
        distances.append((train[x], dist))

    distances.sort(key=operator.itemgetter(1))
    # print(distances)
    neighbours = []

    for x in range(len(distances)):
        if (distances[x][1] < width):
            neighbours.append(distances[x][0])

    return neighbours


# Gaussian product kernel
def kernel(neighborsDic, testInstance, width):
    difference1 = []
    difference0 = []
    length = len(testInstance) - 1
    n1 = 0
    n0 = 0
    for k in neighborsDic:
        if (k == "1"):
            for neighbor in neighborsDic[k]:
                diff1 = get_distance(neighbor, testInstance, length)
                difference1.append((neighbor, diff1))
                n1 += 1
            print("difference is 1" + "\n")
            print(difference1)
            print(n1)
        else:
            for neighbor in neighborsDic[k]:
                diff0 = get_distance(neighbor, testInstance, length)
                difference0.append((neighbor, diff0))
                n0 += 1
            print("difference is 0" + "\n")
            print(difference0)
            print(n0)

    sum1 = 0
    sum0 = 0
    for d in difference1:
        sum1 += math.exp(-(math.pow(d[1] / width, 2)) / 2)
    print("sum1 is: " + "\n")
    print(sum1)

    for d in difference0:
        sum0 += math.exp(-(math.pow(d[1] / width, 2)) / 2)
    print("sum0 is: " + "\n")
    print(sum0)

    width2 = math.pow(width, 2)
    print("width2 is: ", width2)
    nValue1 = n1 * math.pow(2 * width2 * math.pi, length / 2)
    print("nValue1 is: ", nValue1)
    if (nValue1 == 0):
        product1 = 0
    else:
        product1 = 1 / nValue1 * sum1

    nValue0 = n0 * math.pow(2 * width2 * math.pi, length / 2)
    print("nValue0 is: ", nValue0)
    if (nValue0 == 0):
        product0 = 0
    else:
        product0 = 1 / nValue0 * sum0

    print(product1)
    print(product0)

    prob = {}
    prob["1"] = product1
    prob["0"] = product0

    return prob


# make prediction for the test set
def get_response(prior, prob, testInstance):
    r = []
    prob1 = prior["1"] * prob["1"]
    prob0 = prior["0"] * prob["0"]

    totalProb = prob1 + prob0
    if (totalProb == 0):
        freq1 = prob1
        freq0 = prob0
    else:
        freq1 = prob1 / totalProb
        freq0 = prob0 / totalProb

    print(freq1)
    print(freq0)
    if (freq1 <= freq0):
        response = "0"
        r.append((testInstance, response))

    else:
        response = "1"
        r.append((testInstance, response))

    print(r)
    return r


if __name__ == "__main__":
    trainingSet = []
    testSet = []
    split = 0.5
    # data.csv is the dataset "South African Heart Disease";
    # however, it doesn't contain a header line and non-continous value such as famhist
    load_data('data.csv', split, trainingSet, testSet)
    print 'Train set: ' + repr(len(trainingSet))
    print 'Test set: ' + repr(len(testSet))
    # print trainingSet

    separatedClass = separate(trainingSet)
    # print('separatedClass instances: {0}').format(separatedClass)
    prior = priorProb(separatedClass, trainingSet)

    width = 50

    # make prediction for each test set
    responses = []
    correct = 0
    print(testSet)
    for x in range(len(testSet)):
        neighbors = get_neighbours(trainingSet, testSet[x], width)
        print("neighbors" + "\n")
        print(neighbors)

        neighborsClass = separate(neighbors)
        print("neighborsClass" + "\n")
        print(neighborsClass)

        prob = kernel(neighborsClass, testSet[x], width)

        response = get_response(prior, prob, testSet[x])
        print("testSet[x] is:")
        print(testSet[x])

        if (testSet[x][-1] == response[0][1]):
            correct += 1
        responses.append(response)
        print("responses is " + "\n")
        print(responses)

    accuracy = (correct / float(len(testSet))) * 100.0

    # LDA analysis
    trainingSetX = []
    trainingSetY = []
    testSetX = []
    testSetY = []
    for trainx in trainingSet:
        trainy = trainx.pop()
        trainingSetX.append(trainx)
        trainingSetY.append(trainy)
    for testx in testSet:
        testy = testx.pop()
        testSetX.append(testx)
        testSetY.append(testy)
    X = np.array(trainingSetX)
    Y = np.array(trainingSetY)
    lda = LDA()
    lda.fit(X, Y)
    predict = lda.predict(testSetX)
    print(predict)
    testY = np.array(testSetY)
    print(testY)
    accuracyLDA = np.sum(predict == testY) / float(len(testSet)) * 100.0

    print "Naive Bayes Accuracy: " + str(accuracy)
    print "LDA Accuracy: " + str(accuracyLDA)

```

